{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Linear Algebra and Calculus - Recap\n", "\n", "\n", "## Introduction\n", "\n", "Congratulations! You have learned the fundamentals of the math at the core of machine learning: linear algebra and calculus.\n", "\n", "\n", "## Key Takeaways\n", "\n", "### Linear Algebra\n", "\n", "The goal of this part was to provide both a conceptual and computational introduction to linear algebra. Some of the key takeaways include: \n", "\n", "* Scalars, vectors, matrices, and tensors\n", "  * A ***scalar*** is a single, real number\n", "  * A ***vector*** is a one-dimensional array of numbers\n", "  * A ***matrix*** is a 2-dimensional array of numbers\n", "    * Two matrices can be added together if they have the same shape\n", "    * Scalars can be added to matrices by adding the scalar (number) to each element\n", "    * To calculate the dot product for matrix multiplication, the first matrix must have the same number of columns as the number of rows in the second matrix\n", "  * A ***tensor*** is a generalized term for an n-dimensional rectangular grid of numbers. A vector is a one-dimensional (first-order tensor), a matrix is a two-dimensional (second-order tensor), etc.\n", "  * One use case for vectors and matrices is for representing and solving systems of linear equations \n", "* Linear algebra in Python\n", "  * Operating on ***NumPy*** data types is substantially more computationally efficient than performing the same operations on native Python data types\n", "  * It is possible to use linear algebra in NumPy to solve for a linear regression using the ***OLS*** method\n", "  * OLS is not computationally efficient, so in practice, we usually perform a gradient descent instead to solve a linear regression\n", "\n", "### Calculus and Gradient Descent\n", "\n", "The goal of this part was to learn some of the foundational calculus that underpins the gradient descent algorithm. Some of the key takeaways include:\n", "\n", "* Calculus\n", "  * A ***derivative*** is the \"instantaneous rate of change\" of a function - or it can be thought of as the \"slope of the curve\" at a point in time\n", "    * A derivative can also be thought of as a special case of the rate of change over a period of time - as that period of time is zero. \n", "  * If you calculate the rate of change over a period of time and keep reducing the period of time, it usually tends to a limit - which is the value of that derivative\n", "  * Rules can be used for finding derivatives\n", "    * The ***power rule***, ***constant factor rule***, and ***addition rule*** are key tools for calculating derivatives for various kinds of functions\n", "    * The ***chain rule*** can be a useful tool for calculating the derivate of composite functions\n", "* Gradient descent\n", "  * A derivative can be useful for identifying local ***maxima*** or ***minima*** as in both cases, the derivative tends to zero\n", "  * A ***cost curve*** can be used to plot the values of a cost function (in the case of linear regression) for various values of offset and slope for the best fit line\n", "  * ***Gradient descent*** can be used to move towards the local minimum on the cost curve and thus the ideal values for the y-intercept and slope to minimize the selected cost function when performing a linear regression"]}], "metadata": {"kernelspec": {"display_name": "Python (learn-env)", "language": "python", "name": "learn-env"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.5"}}, "nbformat": 4, "nbformat_minor": 2}