{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Algebra and Calculus - Recap\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Congratulations! You have learned the fundamentals of the math at the core of machine learning: linear algebra and calculus.\n",
    "\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "### Linear Algebra\n",
    "\n",
    "The goal of this part was to provide both a conceptual and computational introduction to linear algebra. Some of the key takeaways include: \n",
    "\n",
    "* Scalars, vectors, matrices, and tensors\n",
    "  * A ***scalar*** is a single, real number\n",
    "  * A ***vector*** is a one-dimensional array of numbers\n",
    "  * A ***matrix*** is a 2-dimensional array of numbers\n",
    "    * Two matrices can be added together if they have the same shape\n",
    "    * Scalars can be added to matrices by adding the scalar (number) to each element\n",
    "    * To calculate the dot product for matrix multiplication, the first matrix must have the same number of columns as the number of rows in the second matrix\n",
    "  * A ***tensor*** is a generalized term for an n-dimensional rectangular grid of numbers. A vector is a one-dimensional (first-order tensor), a matrix is a two-dimensional (second-order tensor), etc.\n",
    "  * One use case for vectors and matrices is for representing and solving systems of linear equations \n",
    "* Linear algebra in Python\n",
    "  * Operating on ***NumPy*** data types is substantially more computationally efficient than performing the same operations on native Python data types\n",
    "  * It is possible to use linear algebra in NumPy to solve for a linear regression using the ***OLS*** method\n",
    "  * OLS is not computationally efficient, so in practice, we usually perform a gradient descent instead to solve a linear regression\n",
    "\n",
    "### Calculus and Gradient Descent\n",
    "\n",
    "The goal of this part was to learn some of the foundational calculus that underpins the gradient descent algorithm. Some of the key takeaways include:\n",
    "\n",
    "* Calculus\n",
    "  * A ***derivative*** is the \"instantaneous rate of change\" of a function - or it can be thought of as the \"slope of the curve\" at a point in time\n",
    "    * A derivative can also be thought of as a special case of the rate of change over a period of time - as that period of time is zero. \n",
    "  * If you calculate the rate of change over a period of time and keep reducing the period of time, it usually tends to a limit - which is the value of that derivative\n",
    "  * Rules can be used for finding derivatives\n",
    "    * The ***power rule***, ***constant factor rule***, and ***addition rule*** are key tools for calculating derivatives for various kinds of functions\n",
    "    * The ***chain rule*** can be a useful tool for calculating the derivate of composite functions\n",
    "* Gradient descent\n",
    "  * A derivative can be useful for identifying local ***maxima*** or ***minima*** as in both cases, the derivative tends to zero\n",
    "  * A ***cost curve*** can be used to plot the values of a cost function (in the case of linear regression) for various values of offset and slope for the best fit line\n",
    "  * ***Gradient descent*** can be used to move towards the local minimum on the cost curve and thus the ideal values for the y-intercept and slope to minimize the selected cost function when performing a linear regression"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
